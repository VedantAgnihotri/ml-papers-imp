# Research Papers Implementations

This repository contains my implementations of various machine learning and deep learning research papers, built from scratch. The goal is to replicate the core concepts and algorithms presented in these papers, providing clear, well-commented, and easy-to-understand code.

## Research Papers Implemented

This repository includes implementations of the following research papers:

- **BitNet**: a **1-bit Transformer** architecture for large language models, which aims to scale efficiently in terms of both memory and computation as it employs *low-precision binary weights and quantized activations*.
- **LoRA & Adapter**: a Low-Rank Adaptation technique for fine-tuning large language models, using *low-rank matrices* and *adapter layers* to reduce parameters while maintaining performance. It includes scaling factors to control updates.
  
Each directory corresponds to a specific paper and contains the necessary code to reproduce the results or explore the model.
